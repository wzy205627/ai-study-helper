
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# ---------------------------------------------------------

import streamlit as st
import os
import tempfile
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# é¡µé¢é…ç½®
st.set_page_config(page_title="RAG ç»ˆæå½¢æ€", page_icon="ğŸ§ ")
st.title("ğŸ§  ä½ çš„ç§äººçŸ¥è¯†åº“ (LangChain ç‰ˆ)")

# --- 2. ä¾§è¾¹æ ï¼šAPI Key å’Œ æ–‡ä»¶ä¸Šä¼  ---
with st.sidebar:
    st.header("é…ç½®")
    api_key = st.text_input("SiliconFlow API Key", type="password", placeholder="sk-...")
    
    st.header("ä¸Šä¼ èµ„æ–™")
    uploaded_file = st.file_uploader("ä¸Šä¼  PDF æ–‡ä»¶", type=["pdf"])

# --- 3. æ ¸å¿ƒé€»è¾‘ï¼šå¤„ç†æ–‡ä»¶å¹¶æ„å»º RAG ---
if uploaded_file and api_key:
    # 3.1 ä¸´æ—¶ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆå› ä¸º Loader éœ€è¦è¯»å–æœ¬åœ°æ–‡ä»¶ï¼‰
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name

    # 3.2 åŠ è½½ä¸åˆ‡åˆ†
    with st.spinner("æ­£åœ¨é˜…è¯»æ–‡æ¡£ï¼Œè¯·ç¨å€™..."):
        loader = PyPDFLoader(tmp_file_path)
        docs = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
        splits = text_splitter.split_documents(docs)
        
        # 3.3 å‘é‡åŒ–ä¸å­˜å‚¨ (å…³é”®ï¼šè¿™é‡Œå°±æ˜¯ä½ ä¹‹å‰æ²¡æ‰¾åˆ°çš„é‚£è¡Œä»£ç )
        # æˆ‘ä»¬ä¸æŒ‡å®š persist_directoryï¼Œè®©å®ƒåœ¨å†…å­˜ä¸­è¿è¡Œï¼Œè¿™æ ·äº‘ç«¯æœ€ç¨³å®š
        embeddings = OpenAIEmbeddings(
            api_key=api_key,
            base_url="https://api.siliconflow.cn/v1",
            model="BAAI/bge-m3",
            check_embedding_ctx_length=False,
            chunk_size=50 # é˜²æ­¢ 413 æŠ¥é”™
        )
        
        vectorstore = Chroma.from_documents(
            documents=splits, 
            embedding=embeddings
        )
        retriever = vectorstore.as_retriever()
        
        st.success(f"âœ… æ–‡æ¡£å·²å¤„ç†ï¼Œå…±åˆ‡åˆ†ä¸º {len(splits)} ä¸ªç‰‡æ®µ")

    # --- 4. é—®ç­”é“¾ ---
    # å®šä¹‰æç¤ºè¯æ¨¡æ¿
    prompt = ChatPromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ã€‚
    
    <context>
    {context}
    </context>

    é—®é¢˜ï¼š{input}
    """)
    
    # åˆå§‹åŒ–å¤§æ¨¡å‹
    llm = ChatOpenAI(
        api_key=api_key,
        base_url="https://api.siliconflow.cn/v1",
        model="deepseek-ai/DeepSeek-V3",
        temperature=0.7
    )
    
    # æ„å»ºé“¾
    document_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, document_chain)
    
    # --- 5. èŠå¤©ç•Œé¢ ---
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    # å¤„ç†ç”¨æˆ·æé—®
    if user_input := st.chat_input("å‘æ–‡æ¡£æé—®..."):
        st.chat_message("user").write(user_input)
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        with st.chat_message("assistant"):
            with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
                response = rag_chain.invoke({"input": user_input})
                answer = response["answer"]
                st.write(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

else:
    st.info("ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§å¡«å…¥ API Key å¹¶ä¸Šä¼  PDF æ–‡ä»¶")