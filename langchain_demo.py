import os
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# --- 1. é…ç½® (è¿˜æ˜¯ç”¨ç¡…åŸºæµåŠ¨) ---
# LangChain ä¼šè‡ªåŠ¨å»è¯»ç¯å¢ƒå˜é‡ï¼Œä¹Ÿå¯ä»¥åƒä¸‹é¢è¿™æ ·æ˜¾å¼ä¼ é€’
api_key = "sk-xiewteiwyqvqsaxehcttthserqjbkzyywsmwgaignexanxvl" # <--- âš ï¸ æ¢æˆä½ çš„ Key
base_url = "https://api.siliconflow.cn/v1"

# åˆå§‹åŒ–æ¨¡å‹ (LangChain çš„åŒ…è£…å™¨)
llm = ChatOpenAI(
    api_key=api_key,
    base_url=base_url,
    model="deepseek-ai/DeepSeek-V3",
    temperature=0.7
)

embeddings = OpenAIEmbeddings(
    api_key=api_key,
    base_url=base_url,
    model="BAAI/bge-m3",
    check_embedding_ctx_length=False,
    chunk_size=50 # <--- å…³é”®ï¼è®¾ç½®æ¯æ¬¡åªå‘ 50 ä¸ªç‰‡æ®µï¼Œé¿å¼€ 64 çš„é™åˆ¶
)

# --- 2. åŠ è½½ä¸åˆ‡åˆ† (ä¸€æ°”å‘µæˆ) ---
print("ğŸ“„ æ­£åœ¨åŠ è½½ PDF...")
# âš ï¸ è¿™é‡Œè¯·ç¡®ä¿ä½ ç›®å½•ä¸‹æœ‰ä¸€ä¸ª PDF æ–‡ä»¶ï¼Œæ¯”å¦‚ 'è¯¾åé¢˜.pdf'
# å¦‚æœæ²¡æœ‰ï¼Œè¯·æŠŠä¹‹å‰ç”¨çš„ PDF å¤åˆ¶è¿‡æ¥å¹¶æ”¹ä¸ªç®€å•çš„åå­—
loader = PyPDFLoader("è¯¾åé¢˜ï¼ˆ23ç‰ˆï¼‰.pdf") 
docs = loader.load()

# ä¸“ä¸šçº§åˆ‡åˆ†å™¨ (æ¯”æˆ‘ä»¬ä¹‹å‰æŒ‰å¥å·åˆ‡åˆ†èªæ˜å¤šäº†)
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
splits = text_splitter.split_documents(docs)

print(f"âœ… å·²åˆ‡åˆ†ä¸º {len(splits)} ä¸ªç‰‡æ®µ")

# --- 3. å‘é‡åº“ (ä¸€é”®å…¥åº“) ---
print("ğŸ’¾ æ­£åœ¨å­˜å…¥å‘é‡åº“...")
# from_documents ä¼šè‡ªåŠ¨åš Embedding å¹¶å­˜å…¥ Chroma
vectorstore = Chroma.from_documents(
    documents=splits, 
    embedding=embeddings
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) # æ‰¾å‡ºå‰5å

# --- 4. æ„å»ºé“¾ (The Chain) ---
# è¿™æ˜¯ LangChain æœ€æ ¸å¿ƒçš„æ¦‚å¿µï¼šæŠŠâ€œæ£€ç´¢â€å’Œâ€œç”Ÿæˆâ€ä¸²èµ·æ¥

# å®šä¹‰ Prompt æ¨¡æ¿ (ç³»ç»Ÿä¼šè‡ªåŠ¨æŠŠæ£€ç´¢åˆ°çš„å†…å®¹å¡«å…¥ {context})
prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š
<context>
{context}
</context>

é—®é¢˜ï¼š{input}
""")

# åˆ›å»º "å¡å…¥æ–‡æ¡£é“¾" (Stuff Documents Chain)
# å®ƒçš„ä½œç”¨æ˜¯ï¼šæŠŠæ£€ç´¢åˆ°çš„ 5 æ®µè¯ï¼Œâ€œå¡â€è¿› Prompt é‡Œå‘ç»™ LLM
document_chain = create_stuff_documents_chain(llm, prompt)

# åˆ›å»º "æ£€ç´¢é“¾" (Retrieval Chain)
# å®ƒçš„ä½œç”¨æ˜¯ï¼šæ‹¿åˆ°ç”¨æˆ·é—®é¢˜ -> å»æ£€ç´¢ -> æ‹¿åˆ°ç»“æœ -> æ‰”ç»™ä¸Šé¢çš„ document_chain
rag_chain = create_retrieval_chain(retriever, document_chain)

# --- 5. è¿è¡Œ ---
question = "è¿™ä¸€ç« è®²äº†ä»€ä¹ˆæ ¸å¿ƒæ¦‚å¿µï¼Ÿ"
print(f"\nâ“ æé—®: {question}")

response = rag_chain.invoke({"input": question})

print("\nğŸ¤– AI å›ç­”:")
print(response["answer"])